{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "805af2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path=\"./ckpt/model_mae_78.38278.pt\"  # my best ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a854366c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 55\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pickle\n",
    "import zipfile\n",
    "from datetime import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import *\n",
    "from torch.nn.utils.rnn import *\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, ConcatDataset\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import *\n",
    "\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from IPython.core.debugger import set_trace\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# from sklearn_pandas import DataFrameMapper\n",
    "import sys\n",
    "import wandb\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "\n",
    "import msgpack\n",
    "from box import Box\n",
    "import math\n",
    "import yaml\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "seed_everything(55)\n",
    "\n",
    "\n",
    "from train_utils import train, valid, dict2device\n",
    "# from ds_utils import DsLoader, GisDS, get_train_dl, get_valid_dl\n",
    "from common_utils import *\n",
    "\n",
    "\n",
    "paths = yaml.safe_load(open('./config.yml').read())\n",
    "\n",
    "data_dir = Path(paths['data_dir'])\n",
    "pkl_dir = Path(paths['pkl_dir'])\n",
    "msg_dir = Path(paths['msg_dir'])\n",
    "\n",
    "\n",
    "link_counter = load_pickle(pkl_dir/'link_freq.pkl')\n",
    "len(link_counter)\n",
    "\n",
    "links_30 = [item[0] for item in list(filter(lambda x: x[1]>30, link_counter.most_common()))]\n",
    "len(links_30)\n",
    "\n",
    "\n",
    "link2id = {}\n",
    "for i, link in enumerate(links_30):\n",
    "    link2id[link]=i+1 # 从1开始\n",
    "\n",
    "\n",
    "def clean_linkids(link_ids, link2id):\n",
    "    link_ids = [ str(int(link)) for link in link_ids]\n",
    "    \n",
    "    emb_ids = [link2id[link] if link in link2id else 0 for link in link_ids]\n",
    "    \n",
    "    return emb_ids\n",
    "\n",
    "\n",
    "def batch2tensor(batch, name, log_trans=False, long_tensor=False):\n",
    "    \n",
    "    if long_tensor == True:\n",
    "        x = torch.LongTensor([int(item[name]) for item in batch])\n",
    "    else:\n",
    "        x = torch.FloatTensor([item[name] for item in batch])\n",
    "        \n",
    "    if log_trans==True:\n",
    "        x = torch.log(x)\n",
    "        \n",
    "    return x\n",
    "\n",
    "# 都是log后的\n",
    "dist_min, dist_max, dist_mean, dist_std = (2.4535277755531824, 11.879284286444815, 8.325948361544423, 0.6799133140855674)\n",
    "eta_min, eta_max, eta_mean, eta_std = (2.3978952727983707, 9.371353167823885, 6.553886963677842, 0.5905307292899195)\n",
    "simple_eat_min, simple_eat_max, simple_eat_mean, simple_eat_std = (0.6931471805599453, 9.320180837655714, 6.453206241137908, 0.5758803681400783)\n",
    "\n",
    "high_temp_mean, high_temp_std = (31.84375, 1.6975971069426339)\n",
    "low_temp_mean, low_temp_std = (26.46875, 0.9348922063532245)\n",
    "\n",
    "\n",
    "# 没有log处理\n",
    "link_time_min, link_time_max, link_time_mean, link_time_std = (0.0, 2949.12, 6.843469259130468, 8.63917700058627)\n",
    "\n",
    "\n",
    "driver2id = load_pickle(pkl_dir/\"driver2id_dct.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "90eff7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch, date=None):\n",
    "    order_id = [item['order_id'] for item in batch]\n",
    "    \n",
    "    # numerical\n",
    "    eta = (batch2tensor(batch, 'eta', log_trans=True) - eta_mean)/eta_std\n",
    "    dist = (batch2tensor(batch, 'dist', log_trans=True) - dist_mean)/dist_std\n",
    "    simple_eta = (batch2tensor(batch, 'simple_eta', log_trans=True) - simple_eat_mean)/simple_eat_std\n",
    "    \n",
    "    low_temp = (batch2tensor(batch, 'lowtemp') - low_temp_mean)/low_temp_std\n",
    "    high_temp = (batch2tensor(batch, 'hightemp') - high_temp_mean)/high_temp_std\n",
    "    \n",
    "    driver_id = torch.LongTensor([driver2id[item['driver_id']] for item in batch])\n",
    "    \n",
    "    slice_id = batch2tensor(batch, 'slice_id', long_tensor=True)\n",
    "    \n",
    "    hour = (slice_id*5)//60\n",
    "    \n",
    "    weekday = batch2tensor(batch, 'weekday', long_tensor=True)\n",
    "    \n",
    "    weather = torch.LongTensor([int(item['weather']) for item in batch])\n",
    "    \n",
    "    # link_cross\n",
    "    link_cross_start = [torch.LongTensor(clean_linkids(item['link_id']+item['cross_start'], link2id)) for item in batch]   \n",
    "    link_cross_start = pad_sequence(link_cross_start, batch_first=True)\n",
    "    \n",
    "    link_cross_end = [torch.LongTensor(clean_linkids(item['link_id']+item['cross_end'], link2id)) for item in batch]\n",
    "    link_cross_end = pad_sequence(link_cross_end, batch_first=True)\n",
    "    \n",
    "    link_cross_ratio = [torch.FloatTensor(list(item['link_ratio']) + [1]*len(item['cross_start'])) for item in batch]\n",
    "    link_cross_ratio = pad_sequence(link_cross_ratio, batch_first=True)\n",
    "    \n",
    "    link_cross_current_status = [torch.FloatTensor(list(item['link_current_status']) + [1]*len(item['cross_start'])) for item in batch]\n",
    "    link_cross_current_status = pad_sequence(link_cross_current_status, batch_first=True)/5\n",
    "    \n",
    "    link_cross_len = torch.FloatTensor([ len(item['link_id']) + len(item['cross_start']) for item in batch ])\n",
    "    \n",
    "    link_cross_time = [torch.FloatTensor(item['link_time']+item['cross_time']) for item in batch]\n",
    "    link_cross_time = (pad_sequence(link_cross_time, batch_first=True)-link_time_min)/(link_time_max-link_time_min)\n",
    "\n",
    "    link_time_total = torch.FloatTensor([sum(item['link_time']) for item in batch])/1000\n",
    "    \n",
    "    cross_time_total = torch.FloatTensor([sum(item['cross_time']) for item in batch])/100\n",
    "    \n",
    "    link_len = torch.FloatTensor([len(item['link_time']) for item in batch])/1000\n",
    "    \n",
    "    cross_len = torch.FloatTensor([len(item['cross_time']) for item in batch])/10\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        \"order_id\": order_id,\n",
    "        \"dist\": dist,\n",
    "        \"simple_eta\": simple_eta,\n",
    "        \"driver_id\": driver_id,\n",
    "        \"slice_id\": slice_id,\n",
    "        \"hour\": hour,\n",
    "        \"weekday\": weekday,\n",
    "        \"weather\": weather,\n",
    "        \"low_temp\": low_temp,\n",
    "        \"high_temp\": high_temp,\n",
    "        \n",
    "        \"link_cross_start\": link_cross_start,\n",
    "        \"link_cross_end\": link_cross_end,\n",
    "        \"link_cross_time\": link_cross_time,\n",
    "        \"link_cross_len\": link_cross_len,\n",
    "        \"link_cross_current_status\": link_cross_current_status,\n",
    "        \"link_cross_ratio\": link_cross_ratio,\n",
    "\n",
    "        # 第一波特征\n",
    "        \"link_time_total\": link_time_total,\n",
    "        \"cross_time_total\": cross_time_total,\n",
    "        \"link_len\": link_len,\n",
    "        \"cross_len\": cross_len,\n",
    "        \n",
    "    }, eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a93713ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombineModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 时间\n",
    "        slice_num, slice_dim = 288, 20\n",
    "        driver_num, driver_dim = len(driver2id), 20\n",
    "        \n",
    "        weekday_num, weekday_dim = 7, 3\n",
    "        weather_num, weather_dim = 5, 3\n",
    "        \n",
    "        link_emb_dim = 20 # 目前20最好\n",
    "        link_time_dim = link_ratio_dim = link_current_status_dim = 1\n",
    "\n",
    "        self.link_emb = nn.Embedding(len(link2id)+1, link_emb_dim)\n",
    "        \n",
    "        self.slice_emb = nn.Embedding(slice_num, slice_dim)\n",
    "        \n",
    "        \n",
    "        self.driver_emb = nn.Embedding(driver_num, driver_dim)\n",
    "        \n",
    "        self.weekday_emb = nn.Embedding(weekday_num, weekday_dim)\n",
    "        \n",
    "        self.weather_emb = nn.Embedding(weather_num, weather_dim)\n",
    "        \n",
    "        # link_emb 128 + link_time 1 + link_current_status 1 + link_ratio_dim 1\n",
    "        lstm_input_dim = link_emb_dim + 1 + 1 + 1\n",
    "        lstm_output_dim = 128\n",
    "        self.lstm = LSTM(lstm_input_dim,\n",
    "                         lstm_output_dim, \n",
    "                         batch_first=True,\n",
    "                        )\n",
    "        # ckpt\n",
    "        linear_dim = 175\n",
    "\n",
    "        self.linear = Sequential(Linear(linear_dim,\n",
    "                                        256),\n",
    "                                 LeakyReLU(inplace=True),\n",
    "                                 Linear(256, 128),\n",
    "                                 LeakyReLU(inplace=True),\n",
    "                                 Linear(128, 1)\n",
    "                                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x_link_cross_start = self.link_emb(x['link_cross_start'])\n",
    "        x_link_cross_end = self.link_emb(x['link_cross_end'])\n",
    "        x_link_cross = (x_link_cross_start + x_link_cross_end)/2\n",
    "        \n",
    "        x_link_cross_time = x['link_cross_time']\n",
    "        x_link_cross_current_status = x['link_cross_current_status']\n",
    "        \n",
    "        x_link_cross_ratio = x['link_cross_ratio']\n",
    "        \n",
    "        x_lstm = torch.cat([x_link_cross,\n",
    "                            x_link_cross_time.unsqueeze(-1), \n",
    "                            x_link_cross_current_status.unsqueeze(-1),\n",
    "                            x_link_cross_ratio.unsqueeze(-1),\n",
    "                           ],\n",
    "                           -1)\n",
    "        \n",
    "        packed = pack_padded_sequence(x_lstm,\n",
    "                                      x['link_cross_len'].cpu(), \n",
    "                                      batch_first=True, \n",
    "                                      enforce_sorted=False)\n",
    "        \n",
    "        lstm_output, (ht, ct) = self.lstm(packed)\n",
    "        ht = ht.reshape(len(x['dist']), -1)\n",
    "        \n",
    "        # slice\n",
    "        x_slice = self.slice_emb(x['slice_id'])\n",
    "        \n",
    "        # numerical\n",
    "        x_num = torch.cat([x['simple_eta'].unsqueeze(-1),\n",
    "                           x['dist'].unsqueeze(-1),\n",
    "                           x['low_temp'].unsqueeze(-1),\n",
    "                           x['high_temp'].unsqueeze(-1),\n",
    "\n",
    "                          ], \n",
    "                           axis=-1) # 2\n",
    "        \n",
    "        # driver\n",
    "        x_driver = self.driver_emb(x['driver_id'])\n",
    "        \n",
    "        # weekday\n",
    "        x_weekday = self.weekday_emb(x['weekday'])\n",
    "        \n",
    "        x_comb = torch.cat([ht, x_num, x_slice, x_driver, x_weekday], axis=-1)\n",
    "\n",
    "        res = self.linear(x_comb)\n",
    "\n",
    "        return res.reshape([-1,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c040dbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GisDS(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        try:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                data = f.read()\n",
    "            self.data = msgpack.unpackb(data, use_list=False)\n",
    "        except:\n",
    "            self.data = []\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    \n",
    "def get_train_dl(train_ds, num_workers=0, pin_memory=False, \n",
    "                 collate_fn=None,\n",
    "                 batch_size=3):\n",
    "    \n",
    "    train_dl = DataLoader(train_ds,\n",
    "                          collate_fn=collate_fn, \n",
    "                          batch_size=batch_size,\n",
    "                          num_workers=num_workers,\n",
    "                          pin_memory=pin_memory,\n",
    "                          drop_last=True,\n",
    "                          shuffle=True)\n",
    "    return train_dl\n",
    "\n",
    "def get_valid_dl(valid_ds, num_workers=0, pin_memory=False, \n",
    "                 collate_fn=None,\n",
    "                 batch_size=3):\n",
    "    \n",
    "    val_dl = DataLoader(valid_ds, \n",
    "                        collate_fn=collate_fn, \n",
    "                        batch_size=batch_size,\n",
    "                        num_workers=num_workers,\n",
    "                        pin_memory=pin_memory,\n",
    "                        drop_last=True,\n",
    "                        shuffle=False)\n",
    "    return val_dl\n",
    "\n",
    "class DsLoader():\n",
    "    def __init__(self, cache_all=False):\n",
    "        self.ds_dct = {}\n",
    "        self.cache_all = cache_all\n",
    "        \n",
    "    def get_train_ds(self, day):\n",
    "        if self.cache_all==False:\n",
    "            # del prev day\n",
    "            pre_key = f\"{day-1:02}\"\n",
    "            if pre_key in self.ds_dct:\n",
    "                del self.ds_dct[pre_key]\n",
    "            \n",
    "        key = f\"{day:02}\"\n",
    "        if key not in self.ds_dct:\n",
    "            ds = self.load_ds(day)\n",
    "            self.ds_dct[key] = ds\n",
    "            self.preload_next(day+1)\n",
    "            \n",
    "            return ds\n",
    "        else:\n",
    "            ds = self.ds_dct[key]\n",
    "            self.preload_next(day+1)\n",
    "            return ds\n",
    "        \n",
    "    def preload_next(self, day):\n",
    "        threading.Thread(\n",
    "                    target=self.preload_ds, args=(day,), \n",
    "                    daemon=True\n",
    "            ).start()\n",
    "    \n",
    "    def preload_ds(self, day):\n",
    "        key = f\"{day:02}\"\n",
    "        if key not in self.ds_dct:\n",
    "            self.ds_dct[key]=GisDS(msg_dir/f\"202008{day:02}.msgpack\")\n",
    "        \n",
    "    def load_ds(self, day):\n",
    "        return GisDS(msg_dir/f\"202008{day:02}.msgpack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4f7314b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Box({\n",
    "    \"batch_size\": 512,\n",
    "    \"num_workers\": 20,\n",
    "    \"pin_memory\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "31420725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "model = CombineModel().to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(ckpt_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "57b03bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.04 s, sys: 3.67 s, total: 10.7 s\n",
      "Wall time: 10.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "288076"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "test_ds = GisDS(msg_dir/\"20200901_test.msgpack\")\n",
    "len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0bdb7b62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08899216ac8d48d8acc66f427ef47589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_test_dl(test_ds, collate_fn=None, num_workers=0, pin_memory=False, batch_size=3):\n",
    "    \n",
    "    test_dl = DataLoader(test_ds, \n",
    "                         collate_fn=collate_fn, \n",
    "                         batch_size=batch_size,\n",
    "                         num_workers=num_workers,\n",
    "                         pin_memory=pin_memory,\n",
    "                         drop_last=False,\n",
    "                         shuffle=False)\n",
    "    return test_dl\n",
    "\n",
    "test_dl = get_test_dl(test_ds, \n",
    "                      num_workers=args.num_workers, \n",
    "                      pin_memory=args.pin_memory, \n",
    "                      batch_size=args.batch_size,\n",
    "                      collate_fn=lambda x: collate(x),)\n",
    "\n",
    "res = []\n",
    "\n",
    "for batch in tqdm(test_dl):\n",
    "\n",
    "    pred = torch.exp(\n",
    "        model(dict2device(batch[0], device))*eta_std + eta_mean\n",
    "    )\n",
    "    \n",
    "    res+=pred.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "84054bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df = pd.read_csv(data_dir/\"sample_submission.csv\")\n",
    "\n",
    "submit_df.result = res\n",
    "\n",
    "dt_string = datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n",
    "\n",
    "os.makedirs(\"./predict/\", exist_ok=True)\n",
    " \n",
    "csv_fname = f\"./predict/pred_{dt_string}.csv\"\n",
    "# zip_fname = f\"./predict/pred_{dt_string}.zip\"\n",
    "\n",
    "submit_df.to_csv(csv_fname, index=False)\n",
    "\n",
    "# with zipfile.ZipFile(zip_fname, 'w') as zf:\n",
    "#     zf.write(csv_fname, compress_type=zipfile.ZIP_DEFLATED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70611cde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9c06a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
