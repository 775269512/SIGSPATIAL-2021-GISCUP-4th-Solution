{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "689388e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mjl/anaconda3/envs/torch1.8.1+cuda111/lib/python3.8/site-packages/pytorch_lightning/metrics/__init__.py:43: LightningDeprecationWarning: `pytorch_lightning.metrics.*` module has been renamed to `torchmetrics.*` and split off to its own package (https://github.com/PyTorchLightning/metrics) since v1.3 and will be removed in v1.5\n",
      "  rank_zero_deprecation(\n",
      "Global seed set to 55\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pickle\n",
    "import zipfile\n",
    "from datetime import *\n",
    "import threading\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import *\n",
    "from torch.nn.utils.rnn import *\n",
    "from torch.utils.data import *\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import *\n",
    "\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from IPython.core.debugger import set_trace\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import sys\n",
    "import wandb\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "\n",
    "import msgpack\n",
    "from box import Box\n",
    "import math\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "seed_everything(55)\n",
    "\n",
    "from train_utils import train, valid\n",
    "# from ds_utils import DsLoader, GisDS, get_train_dl, get_valid_dl\n",
    "from common_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85c7416b",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = yaml.safe_load(open('./config.yml').read())\n",
    "\n",
    "data_dir = Path(paths['data_dir'])\n",
    "pkl_dir = Path(paths['pkl_dir'])\n",
    "msg_dir = Path(paths['msg_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9780b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_counter = load_pickle(pkl_dir/'link_freq.pkl')\n",
    "len(link_counter)\n",
    "\n",
    "links_30 = [item[0] for item in list(filter(lambda x: x[1]>30, link_counter.most_common()))]\n",
    "len(links_30)\n",
    "\n",
    "\n",
    "link2id = {}\n",
    "for i, link in enumerate(links_30):\n",
    "    link2id[link]=i+1 # 从1开始\n",
    "\n",
    "\n",
    "def clean_linkids(link_ids, link2id):\n",
    "    link_ids = [ str(int(link)) for link in link_ids]\n",
    "    \n",
    "    emb_ids = [link2id[link] if link in link2id else 0 for link in link_ids]\n",
    "    # emb_ids = [link2id[link] for link in link2id]\n",
    "    \n",
    "    return emb_ids\n",
    "\n",
    "\n",
    "def batch2tensor(batch, name, log_trans=False, long_tensor=False):\n",
    "    \n",
    "    if long_tensor == True:\n",
    "        x = torch.LongTensor([int(item[name]) for item in batch])\n",
    "    else:\n",
    "        x = torch.FloatTensor([item[name] for item in batch])\n",
    "        \n",
    "    if log_trans==True:\n",
    "        x = torch.log(x)\n",
    "        \n",
    "    return x\n",
    "\n",
    "# 都是log后的\n",
    "dist_min, dist_max, dist_mean, dist_std = (2.4535277755531824, 11.879284286444815, 8.325948361544423, 0.6799133140855674)\n",
    "eta_min, eta_max, eta_mean, eta_std = (2.3978952727983707, 9.371353167823885, 6.553886963677842, 0.5905307292899195)\n",
    "simple_eat_min, simple_eat_max, simple_eat_mean, simple_eat_std = (0.6931471805599453, 9.320180837655714, 6.453206241137908, 0.5758803681400783)\n",
    "\n",
    "high_temp_mean, high_temp_std = (31.84375, 1.6975971069426339)\n",
    "low_temp_mean, low_temp_std = (26.46875, 0.9348922063532245)\n",
    "\n",
    "\n",
    "# 没有log处理\n",
    "link_time_min, link_time_max, link_time_mean, link_time_std = (0.0, 2949.12, 6.843469259130468, 8.63917700058627)\n",
    "\n",
    "\n",
    "driver2id = load_pickle(pkl_dir/\"driver2id_dct.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a369ba86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch, date=None):\n",
    "    order_id = [item['order_id'] for item in batch]\n",
    "    \n",
    "    # numerical\n",
    "    eta = (batch2tensor(batch, 'eta', log_trans=True) - eta_mean)/eta_std\n",
    "    dist = (batch2tensor(batch, 'dist', log_trans=True) - dist_mean)/dist_std\n",
    "    simple_eta = (batch2tensor(batch, 'simple_eta', log_trans=True) - simple_eat_mean)/simple_eat_std\n",
    "    \n",
    "    low_temp = (batch2tensor(batch, 'lowtemp') - low_temp_mean)/low_temp_std\n",
    "    high_temp = (batch2tensor(batch, 'hightemp') - high_temp_mean)/high_temp_std\n",
    "    \n",
    "    driver_id = torch.LongTensor([driver2id[item['driver_id']] for item in batch])\n",
    "    \n",
    "    slice_id = batch2tensor(batch, 'slice_id', long_tensor=True)\n",
    "    \n",
    "    hour = (slice_id*5)//60\n",
    "    \n",
    "    weekday = batch2tensor(batch, 'weekday', long_tensor=True)\n",
    "    \n",
    "    weather = torch.LongTensor([int(item['weather']) for item in batch])\n",
    "    \n",
    "    # link_cross\n",
    "    link_cross_start = [torch.LongTensor(clean_linkids(item['link_id']+item['cross_start'], link2id)) for item in batch]   \n",
    "    link_cross_start = pad_sequence(link_cross_start, batch_first=True)\n",
    "    \n",
    "    link_cross_end = [torch.LongTensor(clean_linkids(item['link_id']+item['cross_end'], link2id)) for item in batch]\n",
    "    link_cross_end = pad_sequence(link_cross_end, batch_first=True)\n",
    "    \n",
    "    link_cross_ratio = [torch.FloatTensor(list(item['link_ratio']) + [1]*len(item['cross_start'])) for item in batch]\n",
    "    link_cross_ratio = pad_sequence(link_cross_ratio, batch_first=True)\n",
    "    \n",
    "    link_cross_current_status = [torch.FloatTensor(list(item['link_current_status']) + [1]*len(item['cross_start'])) for item in batch]\n",
    "    link_cross_current_status = pad_sequence(link_cross_current_status, batch_first=True)/5\n",
    "    \n",
    "    link_cross_len = torch.FloatTensor([ len(item['link_id']) + len(item['cross_start']) for item in batch ])\n",
    "    \n",
    "    link_cross_time = [torch.FloatTensor(item['link_time']+item['cross_time']) for item in batch]\n",
    "    link_cross_time = (pad_sequence(link_cross_time, batch_first=True)-link_time_min)/(link_time_max-link_time_min)\n",
    "\n",
    "    link_time_total = torch.FloatTensor([sum(item['link_time']) for item in batch])/1000\n",
    "    \n",
    "    cross_time_total = torch.FloatTensor([sum(item['cross_time']) for item in batch])/100\n",
    "    \n",
    "    link_len = torch.FloatTensor([len(item['link_time']) for item in batch])/1000\n",
    "    \n",
    "    cross_len = torch.FloatTensor([len(item['cross_time']) for item in batch])/10\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        \"order_id\": order_id,\n",
    "        \"dist\": dist,\n",
    "        \"simple_eta\": simple_eta,\n",
    "        \"driver_id\": driver_id,\n",
    "        \"slice_id\": slice_id,\n",
    "        \"hour\": hour,\n",
    "        \"weekday\": weekday,\n",
    "        \"weather\": weather,\n",
    "        \"low_temp\": low_temp,\n",
    "        \"high_temp\": high_temp,\n",
    "        \n",
    "        \"link_cross_start\": link_cross_start,\n",
    "        \"link_cross_end\": link_cross_end,\n",
    "        \"link_cross_time\": link_cross_time,\n",
    "        \"link_cross_len\": link_cross_len,\n",
    "        \"link_cross_current_status\": link_cross_current_status,\n",
    "        \"link_cross_ratio\": link_cross_ratio,\n",
    "\n",
    "        \"link_time_total\": link_time_total,\n",
    "        \"cross_time_total\": cross_time_total,\n",
    "        \"link_len\": link_len,\n",
    "        \"cross_len\": cross_len,\n",
    "        \n",
    "    }, eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffb1e9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombineModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 时间\n",
    "        slice_num, slice_dim = 288, 20\n",
    "        driver_num, driver_dim = len(driver2id), 20\n",
    "        \n",
    "        weekday_num, weekday_dim = 7, 3\n",
    "        weather_num, weather_dim = 5, 3\n",
    "        \n",
    "        link_emb_dim = 20 # 目前20最好\n",
    "        link_time_dim = link_ratio_dim = link_current_status_dim = 1\n",
    "\n",
    "        self.link_emb = nn.Embedding(len(link2id)+1, link_emb_dim)\n",
    "        \n",
    "        self.slice_emb = nn.Embedding(slice_num, slice_dim)\n",
    "        \n",
    "        \n",
    "        self.driver_emb = nn.Embedding(driver_num, driver_dim)\n",
    "        \n",
    "        self.weekday_emb = nn.Embedding(weekday_num, weekday_dim)\n",
    "        \n",
    "        self.weather_emb = nn.Embedding(weather_num, weather_dim)\n",
    "        \n",
    "        # link_emb 128 + link_time 1 + link_current_status 1 + link_ratio_dim 1\n",
    "        lstm_input_dim = link_emb_dim + 1 + 1 + 1\n",
    "        lstm_output_dim = 128\n",
    "        self.lstm = LSTM(lstm_input_dim,\n",
    "                         lstm_output_dim, \n",
    "                         batch_first=True,\n",
    "                        )\n",
    "        # ckpt\n",
    "        linear_dim = 175\n",
    "\n",
    "        self.linear = Sequential(Linear(linear_dim,\n",
    "                                        256),\n",
    "                                 LeakyReLU(inplace=True),\n",
    "                                 Linear(256, 128),\n",
    "                                 LeakyReLU(inplace=True),\n",
    "                                 Linear(128, 1)\n",
    "                                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x_link_cross_start = self.link_emb(x['link_cross_start'])\n",
    "        x_link_cross_end = self.link_emb(x['link_cross_end'])\n",
    "        x_link_cross = (x_link_cross_start + x_link_cross_end)/2\n",
    "        \n",
    "        x_link_cross_time = x['link_cross_time']\n",
    "        x_link_cross_current_status = x['link_cross_current_status']\n",
    "        \n",
    "        x_link_cross_ratio = x['link_cross_ratio']\n",
    "        \n",
    "        x_lstm = torch.cat([x_link_cross,\n",
    "                            x_link_cross_time.unsqueeze(-1), \n",
    "                            x_link_cross_current_status.unsqueeze(-1),\n",
    "                            x_link_cross_ratio.unsqueeze(-1),\n",
    "                           ],\n",
    "                           -1)\n",
    "        \n",
    "        packed = pack_padded_sequence(x_lstm,\n",
    "                                      x['link_cross_len'].cpu(), \n",
    "                                      batch_first=True, \n",
    "                                      enforce_sorted=False)\n",
    "        \n",
    "        lstm_output, (ht, ct) = self.lstm(packed)\n",
    "        ht = ht.reshape(len(x['dist']), -1)\n",
    "        \n",
    "        # slice\n",
    "        x_slice = self.slice_emb(x['slice_id'])\n",
    "        \n",
    "        # numerical\n",
    "        x_num = torch.cat([x['simple_eta'].unsqueeze(-1),\n",
    "                           x['dist'].unsqueeze(-1),\n",
    "                           x['low_temp'].unsqueeze(-1),\n",
    "                           x['high_temp'].unsqueeze(-1),\n",
    "\n",
    "                          ], \n",
    "                           axis=-1) # 2\n",
    "        \n",
    "        # driver\n",
    "        x_driver = self.driver_emb(x['driver_id'])\n",
    "        \n",
    "        # weekday\n",
    "        x_weekday = self.weekday_emb(x['weekday'])\n",
    "        \n",
    "        x_comb = torch.cat([ht, x_num, x_slice, x_driver, x_weekday], axis=-1)\n",
    "\n",
    "        res = self.linear(x_comb)\n",
    "\n",
    "        return res.reshape([-1,])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e088fd79",
   "metadata": {},
   "source": [
    "## args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6a41669",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Box({\n",
    "    \"batch_size\": 512,\n",
    "    \"lr\": 1e-4,\n",
    "    \"epochs\": 50,\n",
    "    \"num_workers\": 10,\n",
    "    \"pin_memory\": True,\n",
    "    \"wandb_mode\": None, #\"offline\"\n",
    "    \"save_code\": True,\n",
    "    \"save_ckpt\": True,\n",
    "    \"train_day\": (2, 31),\n",
    "    \"val_day\": 1,\n",
    "    \"mixed\": True,\n",
    "    \"run_name\": \"for compt\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b81def21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GisDS(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        try:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                data = f.read()\n",
    "            self.data = msgpack.unpackb(data, use_list=False)\n",
    "        except:\n",
    "            self.data = []\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    \n",
    "def get_train_dl(train_ds, num_workers=0, pin_memory=False, \n",
    "                 collate_fn=None,\n",
    "                 batch_size=3):\n",
    "    \n",
    "    train_dl = DataLoader(train_ds,\n",
    "                          collate_fn=collate_fn, \n",
    "                          batch_size=batch_size,\n",
    "                          num_workers=num_workers,\n",
    "                          pin_memory=pin_memory,\n",
    "                          drop_last=True,\n",
    "                          shuffle=True)\n",
    "    return train_dl\n",
    "\n",
    "def get_valid_dl(valid_ds, num_workers=0, pin_memory=False, \n",
    "                 collate_fn=None,\n",
    "                 batch_size=3):\n",
    "    \n",
    "    val_dl = DataLoader(valid_ds, \n",
    "                        collate_fn=collate_fn, \n",
    "                        batch_size=batch_size,\n",
    "                        num_workers=num_workers,\n",
    "                        pin_memory=pin_memory,\n",
    "                        drop_last=True,\n",
    "                        shuffle=False)\n",
    "    return val_dl\n",
    "\n",
    "class DsLoader():\n",
    "    def __init__(self, cache_all=False):\n",
    "        self.ds_dct = {}\n",
    "        self.cache_all = cache_all\n",
    "        \n",
    "    def get_train_ds(self, day):\n",
    "        if self.cache_all==False:\n",
    "            # del prev day\n",
    "            pre_key = f\"{day-1:02}\"\n",
    "            if pre_key in self.ds_dct:\n",
    "                del self.ds_dct[pre_key]\n",
    "            \n",
    "        key = f\"{day:02}\"\n",
    "        if key not in self.ds_dct:\n",
    "            ds = self.load_ds(day)\n",
    "            self.ds_dct[key] = ds\n",
    "            self.preload_next(day+1)\n",
    "            \n",
    "            return ds\n",
    "        else:\n",
    "            ds = self.ds_dct[key]\n",
    "            self.preload_next(day+1)\n",
    "            return ds\n",
    "        \n",
    "    def preload_next(self, day):\n",
    "        threading.Thread(\n",
    "                    target=self.preload_ds, args=(day,), \n",
    "                    daemon=True\n",
    "            ).start()\n",
    "    \n",
    "    def preload_ds(self, day):\n",
    "        key = f\"{day:02}\"\n",
    "        if key not in self.ds_dct:\n",
    "            self.ds_dct[key]=GisDS(msg_dir/f\"202008{day:02}.msgpack\")\n",
    "        \n",
    "    def load_ds(self, day):\n",
    "        return GisDS(msg_dir/f\"202008{day:02}.msgpack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e93bd5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ds = GisDS(msg_dir/f'202008{args.val_day:02}.msgpack')\n",
    "\n",
    "valid_dl = get_valid_dl(valid_ds,\n",
    "                        collate_fn = lambda x: collate(x),\n",
    "                        num_workers=args.num_workers, \n",
    "                        pin_memory=args.pin_memory,\n",
    "                        batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ef0518e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdawnywu\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.31<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">for compt</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/dawnywu/learn\" target=\"_blank\">https://wandb.ai/dawnywu/learn</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/dawnywu/learn/runs/2vbsekng\" target=\"_blank\">https://wandb.ai/dawnywu/learn/runs/2vbsekng</a><br/>\n",
       "                Run data is saved locally in <code>/home/mjl/notebooks/competitions/giscup_2021/to_compet/wandb/run-20210821_205458-2vbsekng</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98566b98f13149d2a11abd01f00be4e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5207378d32b48a6aadd5def058aabc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-916ed0858d2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mtrain_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_train_ds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         train_dl = get_train_dl(train_ds,\n\u001b[0m\u001b[1;32m     35\u001b[0m                                 \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                                 \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-a13f9ba91d94>\u001b[0m in \u001b[0;36mget_train_dl\u001b[0;34m(train_ds, num_workers, pin_memory, collate_fn, batch_size)\u001b[0m\n\u001b[1;32m     19\u001b[0m                  batch_size=3):\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     train_dl = DataLoader(train_ds,\n\u001b[0m\u001b[1;32m     22\u001b[0m                           \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch1.8.1+cuda111/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers)\u001b[0m\n\u001b[1;32m    264\u001b[0m                     \u001b[0;31m# Cannot statically verify that dataset is Sized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                     \u001b[0;31m# Somewhat related: see NOTE [ Lack of Default `__len__` in Python Abstract Base Classes ]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch1.8.1+cuda111/lib/python3.8/site-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             raise ValueError(\"num_samples should be a positive integer \"\n\u001b[0m\u001b[1;32m    104\u001b[0m                              \"value, but got num_samples={}\".format(self.num_samples))\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "model = CombineModel().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.98)\n",
    "\n",
    "ds_loader = DsLoader(cache_all=False)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "wandb.init(project=\"learn\",\n",
    "           name=args.run_name,\n",
    "           config=args,\n",
    "           mode=args.wandb_mode,\n",
    "           save_code=True,\n",
    "          )\n",
    "\n",
    "wandb.log({\"grad_params_num\": count_parameters(model)})\n",
    "\n",
    "mae_min=999.\n",
    "mape_min=999.\n",
    "tqdm_epoch = tqdm(range(args.epochs))\n",
    "for epoch_idx in tqdm_epoch:\n",
    "    tqdm_epoch.set_description(f\"Epoch {epoch_idx}\")\n",
    "    # 以后日期放到args里？\n",
    "    tqdm_date = tqdm(range(args.train_day[0], args.train_day[1]+1), leave=False)\n",
    "    for date in tqdm_date:\n",
    "        if date==3: continue\n",
    "        tqdm_date.set_description(f\"Date 202008{date:02}\")\n",
    "        \n",
    "        date_string = f\"202008{date:02}\"\n",
    "        \n",
    "        train_ds = ds_loader.get_train_ds(date)\n",
    "        train_dl = get_train_dl(train_ds,\n",
    "                                num_workers=args.num_workers, \n",
    "                                pin_memory=args.pin_memory, \n",
    "                                collate_fn = lambda x: collate(x, date_string),\n",
    "                                batch_size=args.batch_size)\n",
    "\n",
    "        train(epoch_idx, date, model, train_dl, optimizer, device, mixed=args.mixed);\n",
    "    valid(model, valid_dl, device, save_ckpt=args.save_ckpt);\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccca2275",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0212f15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
